{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fc05ef",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8142eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa442ab9",
   "metadata": {},
   "source": [
    "## Step 1: Sample Text Data\n",
    "### Assume you have a small corpus of documents (sentences or paragraphs) for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b5aa7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"dogs and cats living together\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ed554",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization\n",
    "### Tokenize the documents into individual words. This step also involves converting all the words to lowercase to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "131a0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents = [doc.lower().split() for doc in documents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04003b9a",
   "metadata": {},
   "source": [
    "## Step 3: Build a Vocabulary\n",
    "### Create a list of unique words used across all documents. This will serve as the columns in your BoW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63afe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(set(word for doc in tokenized_documents for word in doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94ab49",
   "metadata": {},
   "source": [
    "## Step 4: Generate BoW Vectors\n",
    "### For each document, create a vector where each element counts how many times a word from the vocabulary appears in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e7d1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a matrix of zeros with dimensions (number of documents) x (vocabulary size)\n",
    "bow_vectors = np.zeros((len(documents), len(vocabulary)), dtype=int)\n",
    "\n",
    "# Populate the matrix with word counts\n",
    "for i, doc in enumerate(tokenized_documents):\n",
    "    for word in doc:\n",
    "        bow_vectors[i, vocabulary.index(word)] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc55bcd",
   "metadata": {},
   "source": [
    "## Step 5: Display the BoW Model\n",
    "### You can display the BoW model to see the vectors or use them for further analysis, such as training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f87d5ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'cat', 'cats', 'dog', 'dogs', 'living', 'log', 'mat', 'on', 'sat', 'the', 'together']\n",
      "BoW Vectors:\n",
      " [[0 1 0 0 0 0 0 1 1 1 2 0]\n",
      " [0 0 0 1 0 0 1 0 1 1 2 0]\n",
      " [1 0 1 0 1 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"BoW Vectors:\\n\", bow_vectors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
